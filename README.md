# Text_Tokenization
Demo for text tokenization using spaCy

Tokenization, in Natural Language Processing (NLP) and machine learning, refers to converting a sequence of text into smaller parts, known as tokens. These tokens can be as small as characters or as long as words!
